{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mc8f-B1BQ4Qq"
   },
   "source": [
    "# Lab. 6: Bayesian Optimization\n",
    "\n",
    "## Introduction\n",
    "\n",
    "#### <u>The goal of this lab is to study the behavior of Bayesian optimization on a regression problem.</u>\n",
    "\n",
    "Bayesian optimization is a probabilistic approach that uses the Bayes' Theorem $P(A|B) = \\frac{P(B|A)*P(A)}{P(B)}$. Briefly, we use the prior information, P(A),(random samples) to optimize a surrogate function, P(B|A).\n",
    "\n",
    "[Here](https://scikit-learn.org/1.2/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor.predict) you can find the \"Gaussian Process Regressor\" class documentation.\n",
    "\n",
    "You'll have to implement some methods:\n",
    "\n",
    "- *Acquisition Functions*:\n",
    "  - `USB(args)` Upper Confidence Bound\n",
    "  $$ \\mu(x) + k \\sigma(x)  $$\n",
    "  - `LSB(args)` Lower Confidence Bound (for minimization only)\n",
    "  $$ \\mu(x) - k \\sigma(x)  $$\n",
    "  - `PI(args)` Probability of Improvement\n",
    "  $$ P(f(x) + k \\sigma(x))  $$\n",
    "  namely\n",
    "  $$ 1-\\Phi(z_0) = \\Phi(-z_0) = \\Phi(-\\frac{f(x^*) + k - \\mu(x)}{\\sigma(x)}) $$\n",
    "  where $\\Phi$ is the normal cumulative distribution function (hint: use [scipy.stats.norm](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html#scipy-stats-norm)).\n",
    "  - `EI(args)` Expected Improvement\n",
    "  $$ EI(x) = E[f(x) - f(x_t^+)] $$\n",
    "  namely\n",
    "  $$ EI(x) = (\\mu(x) - f(x^*) - k)\\Phi(\\frac{\\mu(x) - f(x^*) - k}{\\sigma(x)}) + \\sigma(x)\\varphi(\\frac{f(x^*) + k - \\mu(x)}{\\sigma(x)}) $$\n",
    "  where $\\Phi$ is the normal cumulative distribution function and $\\varphi$ is the probability density function (hint: use [scipy.stats.norm](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html#scipy-stats-norm))\n",
    "\n",
    "- *Prior Functions*:\n",
    "  - `Prior_1(objective_fun, size)` Where `X` is a random array of size `size` and `Y` is the array of `f(x)`.\n",
    "  - `Prior_2(objective_fun, size)` Where `X` is an array of size `size` of numbers sampled from a uniform distribution `[0.5, 1]` and `Y` is the array of `f(x)`.\n",
    "  - `Prior_3(objective_fun, size)` Where `X` is an array of size `size` of numbers sampled from a uniform distribution `[0, 0.5]` and `Y` is the array of `f(x)`.\n",
    "- *Optimization Functions*. Here you can find some examples, feel free to use different functions:\n",
    "  - `multimodal_1(x)`\n",
    "  $$ \\sin(x) + \\sin(\\frac{10}{3}x) $$\n",
    "  - `multimodal_2(x)`\n",
    "  $$ (1.4-3.0x) \\sin(18x) $$\n",
    "  - `multimodal_3(x)`\n",
    "  $$ x^2 + \\sin(5 \\pi x)^6 $$\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mo4CeiMJX7wR"
   },
   "outputs": [],
   "source": [
    "# example of bayesian optimization for a 1d function from scratch\n",
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "from typing import Optional\n",
    "from warnings import catch_warnings, simplefilter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from numpy.typing import NDArray\n",
    "from scipy.stats import norm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, DotProduct, ExpSineSquared, Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XChZ039pNinR"
   },
   "outputs": [],
   "source": [
    "class AcquisitionFunctionName(Enum):\n",
    "    UCB = \"ucb\"\n",
    "    LCB = \"lcb\"\n",
    "    PI = \"pi\"\n",
    "    EI = \"ei\"\n",
    "\n",
    "\n",
    "class AcquisitionFunction(ABC):\n",
    "    def __init__(\n",
    "        self, mu: NDArray[np.float64], std: NDArray[np.float64], best: float, k: float\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Abstract class for acquisition functions\n",
    "        - mu: array of mean values for each sample\n",
    "        - std: array of standard deviation values for each sample\n",
    "        - best: best value found so far\n",
    "        - k: exploration parameter\n",
    "\n",
    "        \"\"\"\n",
    "        self.mu = mu\n",
    "        self.std = std\n",
    "        self.best = best\n",
    "        self.k = k\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self) -> NDArray[np.float64]:\n",
    "        pass\n",
    "\n",
    "\n",
    "class UCB(AcquisitionFunction):\n",
    "    \"\"\"\n",
    "    Upper Confidence Bound implementation\n",
    "    - mu: array of mean values for each sample\n",
    "    - std: array of standard deviation values for each sample\n",
    "    - best: best value found so far\n",
    "    - k: exploration parameter\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self) -> NDArray[np.float64]:\n",
    "        return self.mu + self.k * self.std\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"UCB = mu + k * std\"\n",
    "\n",
    "\n",
    "class LCB(AcquisitionFunction):\n",
    "    \"\"\"\n",
    "    Lower Confidence Bound implementation\n",
    "    - mu: array of mean values for each sample\n",
    "    - std: array of standard deviation values for each sample\n",
    "    - best: best value found so far\n",
    "    - k: exploration parameter\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self) -> NDArray[np.float64]:\n",
    "        return self.mu - self.k * self.std\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"LCB = mu - k * std\"\n",
    "\n",
    "\n",
    "class PI(AcquisitionFunction):\n",
    "    \"\"\"\n",
    "    Probability of implementation\n",
    "    - mu: array of mean values for each sample\n",
    "    - std: array of standard deviation values for each sample\n",
    "    - best: best value found so far\n",
    "    - k: exploration parameter\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self) -> NDArray[np.float64]:\n",
    "        return norm.cdf(-(self.best + self.k - self.mu) / self.std)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"PI = norm.cdf(-(best + k - mu) / std)\"\n",
    "\n",
    "\n",
    "class EI(AcquisitionFunction):\n",
    "    def __init__(\n",
    "        self, mu: NDArray[np.float64], std: NDArray[np.float64], best: float, k: float\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Expected improvement implementation\n",
    "        - mu: array of mean values for each sample\n",
    "        - std: array of standard deviation values for each sample\n",
    "        - best: best value found so far\n",
    "        - k: exploration parameter\n",
    "\n",
    "        \"\"\"\n",
    "        self.mu = mu\n",
    "        self.std = std\n",
    "        self.best = best\n",
    "        self.k = k\n",
    "\n",
    "    def __call__(self) -> NDArray[np.float64]:\n",
    "        return (self.mu - self.best - self.k) * norm.cdf(\n",
    "            (self.mu - self.best - self.k) / self.std\n",
    "        ) + self.std * norm.pdf((self.best + self.k - self.mu) / self.std)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"(mu - best - k) * norm.cdf((mu - best - k) / std) + std * norm.pdf((best + k - mu) / std)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectiveFunctionName(Enum):\n",
    "    Multimodal_1 = \"multimodal_1\"\n",
    "    Multimodal_2 = \"multimodal_2\"\n",
    "    Multimodal_3 = \"multimodal_3\"\n",
    "\n",
    "\n",
    "class ObjectiveFunction(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, x: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        pass\n",
    "\n",
    "\n",
    "class Multimodal_1(ObjectiveFunction):\n",
    "    def __init__(self, noise: float):\n",
    "        self.noise = noise\n",
    "\n",
    "    def __call__(self, x: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        \"\"\"\n",
    "        Implementation of the multimodal function: sin(x) + sin(10/3 * x)\n",
    "        - x: value to be evaluated\n",
    "\n",
    "        Returns:\n",
    "        - y: evaluation of x\n",
    "        \"\"\"\n",
    "        return self.f(x) + npr.uniform(low=-self.noise, high=self.noise)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"sin(x) + sin(10/3 * x)\"\n",
    "\n",
    "    def f(self, x: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        return np.sin(x) - np.sin(10 / 3 * x)\n",
    "\n",
    "    @property\n",
    "    def max(self) -> tuple[float, float]:\n",
    "        \"returns the max in the [0,1] interval\"\n",
    "        return (1.0, 1.0)\n",
    "\n",
    "\n",
    "class Multimodal_2(ObjectiveFunction):\n",
    "    def __init__(self, noise: float):\n",
    "        self.noise = noise\n",
    "\n",
    "    def __call__(self, x: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        \"\"\"\n",
    "        Implementation of the multimodal function: (1.4 - 3.0 * x) * sin(18 * x)\n",
    "        - x: value to be evaluated\n",
    "\n",
    "        Returns:\n",
    "        - y: evaluation of x\n",
    "        \"\"\"\n",
    "        return self.f(x) + npr.uniform(low=-self.noise, high=self.noise)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"(1.4 - 3.0 * x) * sin(18 * x)\"\n",
    "\n",
    "    def f(self, x: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        return (1.4 - 3.0 * x) * np.sin(18 * x)\n",
    "\n",
    "    @property\n",
    "    def max(self) -> tuple[float, float]:\n",
    "        \"returns the max in the [0,1] interval\"\n",
    "        return (0.966, 1.489)\n",
    "\n",
    "\n",
    "class Multimodal_3(ObjectiveFunction):\n",
    "    def __init__(self, noise: float):\n",
    "        self.noise = noise\n",
    "\n",
    "    def __call__(self, x: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        \"\"\"\n",
    "        Implementation of the multimodal function: x^2 + sin(5 * pi * x)^6\n",
    "        - x: value to be evaluated\n",
    "\n",
    "        Returns:\n",
    "        - y: evaluation of x\n",
    "        \"\"\"\n",
    "        return self.f(x) + npr.uniform(low=-self.noise, high=self.noise)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"x^2 + sin(5 * pi * x)^6\"\n",
    "\n",
    "    def f(self, x: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        return x**2 + np.sin(5 * np.pi * x) ** 6\n",
    "\n",
    "    @property\n",
    "    def max(self) -> tuple[float, float]:\n",
    "        \"returns the max in the [0,1] interval\"\n",
    "        return (0.9, 1.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bq78Ws8tOBF3"
   },
   "outputs": [],
   "source": [
    "class PriorName(Enum):\n",
    "    Prior_1 = \"prior_1\"\n",
    "    Prior_2 = \"prior_2\"\n",
    "    Prior_3 = \"prior_3\"\n",
    "\n",
    "\n",
    "class Prior(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self, objective_fun: ObjectiveFunction, size: int):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self) -> tuple[NDArray[np.float64], NDArray[np.float64]]:\n",
    "        pass\n",
    "\n",
    "\n",
    "class Prior_1(Prior):\n",
    "    def __init__(self, objective_fun: ObjectiveFunction, size: int):\n",
    "        \"\"\"\n",
    "        - objective_fun: function that implements the objective function in use\n",
    "        - size: an Integer that specifies the size of X and Y arrays\n",
    "        \"\"\"\n",
    "        self.objective_fun = objective_fun\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self) -> tuple[NDArray[np.float64], NDArray[np.float64]]:\n",
    "        \"\"\"\n",
    "        Prior implementation, returns a tuple (X, Y), where `X` is a random array of\n",
    "        size `size` and `Y` is the array of `f(x)`.\n",
    "\n",
    "        Returns:\n",
    "        - X, Y\n",
    "        \"\"\"\n",
    "        sample = npr.random(self.size)\n",
    "        evaluations = [self.objective_fun(x) for x in sample]\n",
    "        return sample, np.asarray(evaluations)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"Unif(0,1)\"\n",
    "\n",
    "\n",
    "class Prior_2(Prior):\n",
    "    def __init__(self, objective_fun: ObjectiveFunction, size: int):\n",
    "        \"\"\"\n",
    "        - objective_fun: function that implements the objective function in use\n",
    "        - size: an Integer that specifies the size of X and Y arrays\n",
    "        \"\"\"\n",
    "        self.objective_fun = objective_fun\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self) -> tuple[NDArray[np.float64], NDArray[np.float64]]:\n",
    "        \"\"\"\n",
    "        Prior implementation, returns a tuple (X, Y), where `X` is an array of size `size`\n",
    "        of numbers sampled from a uniform distribution `[0.5, 1]` and `Y` is the array of `f(x)`.\n",
    "\n",
    "        Returns:\n",
    "        - X, Y\n",
    "        \"\"\"\n",
    "        sample = npr.uniform(0.5, 1, self.size)\n",
    "        evaluations = [self.objective_fun(x) for x in sample]\n",
    "        return sample, np.asarray(evaluations)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"Unif(0.5,1)\"\n",
    "\n",
    "\n",
    "class Prior_3(Prior):\n",
    "    def __init__(self, objective_fun: ObjectiveFunction, size: int):\n",
    "        \"\"\"\n",
    "        - objective_fun: function that implements the objective function in use\n",
    "        - size: an Integer that specifies the size of X and Y arrays\n",
    "        \"\"\"\n",
    "        self.objective_fun = objective_fun\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self) -> tuple[NDArray[np.float64], NDArray[np.float64]]:\n",
    "        \"\"\"\n",
    "        Prior implementation, returns a tuple (X, Y), where `X` is an array of size `size`\n",
    "        of numbers sampled from a uniform distribution `[0, 0.5]` and `Y` is the array of `f(x)`.\n",
    "\n",
    "        Returns:\n",
    "        - X, Y\n",
    "        \"\"\"\n",
    "        sample = npr.uniform(0, 0.5, self.size)\n",
    "        evaluations = [self.objective_fun(x) for x in sample]\n",
    "        return sample, np.asarray(evaluations)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"Unif(0,0.5)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianOptimization:\n",
    "    def __init__(\n",
    "        self,\n",
    "        generation: int,\n",
    "        size: int,\n",
    "        acquisition_function: AcquisitionFunctionName,\n",
    "        objective_fun: ObjectiveFunctionName,\n",
    "        prior: PriorName,\n",
    "        noise: float = 0.1,\n",
    "        kernel: Optional[Kernel] = None,\n",
    "        k: float = 0,\n",
    "    ) -> None:\n",
    "        self.generation = generation\n",
    "        self.size = size\n",
    "\n",
    "        self._acquisition_function_name = acquisition_function\n",
    "\n",
    "        match objective_fun.value:\n",
    "            case ObjectiveFunctionName.Multimodal_1.value:\n",
    "                self.objective_fun = Multimodal_1(noise)\n",
    "            case ObjectiveFunctionName.Multimodal_2.value:\n",
    "                self.objective_fun = Multimodal_2(noise)\n",
    "            case ObjectiveFunctionName.Multimodal_3.value:\n",
    "                self.objective_fun = Multimodal_3(noise)\n",
    "            case _:  # type: ignore\n",
    "                raise ValueError(\"Invalid objective function\")\n",
    "\n",
    "        match prior.value:\n",
    "            case PriorName.Prior_1.value:\n",
    "                self.prior = Prior_1(self.objective_fun, size)\n",
    "            case PriorName.Prior_2.value:\n",
    "                self.prior = Prior_2(self.objective_fun, size)\n",
    "            case PriorName.Prior_3.value:\n",
    "                self.prior = Prior_3(self.objective_fun, size)\n",
    "            case _:  # type: ignore\n",
    "                raise ValueError(\"Invalid prior\")\n",
    "\n",
    "        self.model = GaussianProcessRegressor(kernel)\n",
    "        self.k = k\n",
    "\n",
    "    def __call__(self) -> tuple[NDArray[np.float64], NDArray[np.float64]]:\n",
    "        # sample the domain sparsely with noise\n",
    "        x, y = self.prior.__call__()\n",
    "\n",
    "        # reshape into rows and cols\n",
    "        x = x.reshape(len(x), 1)\n",
    "        y = y.reshape(len(y), 1)\n",
    "\n",
    "        # fit the model\n",
    "        self.model.fit(x, y)\n",
    "        # perform the optimization process\n",
    "        for _ in range(self.generation):\n",
    "            # select the next point to sample\n",
    "            x_p = self._acquisition_optimization(x, y)\n",
    "            # sample the point\n",
    "            actual = self.objective_fun(x_p)\n",
    "            # summarize the finding\n",
    "            self.surrogate(np.array([[x_p]]))\n",
    "            # add the data to the dataset\n",
    "            x = np.vstack((x, [[x_p]]))\n",
    "            y = np.vstack((y, [[actual]]))\n",
    "            # update the model\n",
    "            self.model.fit(x, y)\n",
    "        return x, y\n",
    "\n",
    "    # surrogate or approximation for the objective function\n",
    "    def surrogate(\n",
    "        self, X: NDArray[np.float64]\n",
    "    ) -> tuple[NDArray[np.float64], NDArray[np.float64]]:\n",
    "        # catch any warning generated when making a prediction\n",
    "        with catch_warnings():\n",
    "            # ignore generated warnings\n",
    "            simplefilter(\"ignore\")\n",
    "            return self.model.predict(X, return_std=True)  # type: ignore\n",
    "\n",
    "    # plot real observations vs surrogate function\n",
    "    def plot(self, X: NDArray[np.float64], y: NDArray[np.float64]):\n",
    "        ax: plt.Axes\n",
    "        f, ax = plt.subplots(1, 1)\n",
    "\n",
    "        # scatter plot of inputs and real objective function\n",
    "        ax.scatter(X[: self.size], y[: self.size], marker=\"^\", label=\"surrogate points\")  # type: ignore\n",
    "        ax.scatter(X[self.size :], y[self.size :], color=\"red\", label=\"real points\")  # type: ignore\n",
    "        # line plot of surrogate function across domain\n",
    "        Xsamples = np.asarray(np.arange(0, 1, 0.001))\n",
    "        Xsamples = Xsamples.reshape(len(Xsamples), 1)\n",
    "        ysamples, _ = self.surrogate(Xsamples)\n",
    "        ax.plot(Xsamples, ysamples, label=\"surrogate function\")\n",
    "        # plot real objective function with noise\n",
    "\n",
    "        ysamples = np.asarray([self.objective_fun.f(x) for x in Xsamples])\n",
    "        ax.plot(Xsamples, ysamples, \"r--\", label=\"real function\")\n",
    "        ax.fill_between(\n",
    "            Xsamples.flatten(),\n",
    "            ysamples.flatten() + self.objective_fun.noise,  # type: ignore\n",
    "            ysamples.flatten() - self.objective_fun.noise,  # type: ignore\n",
    "            color=\"red\",\n",
    "            alpha=0.3,\n",
    "        )\n",
    "        ax.scatter(\n",
    "            *self.objective_fun.max, color=\"green\", marker=\"x\", label=\"best point\"\n",
    "        )\n",
    "        f.suptitle(\n",
    "            f\"BayesianOptimization for function {self.objective_fun} \\n with prior {self.prior}and acquisition function {self._acquisition_function_name.name}\"\n",
    "        )\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # create the acquisition function\n",
    "    def _acquisition_function(\n",
    "        self,\n",
    "        function: AcquisitionFunctionName,\n",
    "        mu: NDArray[np.float64],\n",
    "        std: NDArray[np.float64],\n",
    "        best: float,\n",
    "        k: float,\n",
    "    ) -> NDArray[np.float64]:\n",
    "        match function.value:\n",
    "            case AcquisitionFunctionName.UCB.value:\n",
    "                self.acquisition_function = UCB(mu, std, best, k)\n",
    "            case AcquisitionFunctionName.LCB.value:\n",
    "                self.acquisition_function = LCB(mu, std, best, k)\n",
    "            case AcquisitionFunctionName.PI.value:\n",
    "                self.acquisition_function = PI(mu, std, best, k)\n",
    "            case AcquisitionFunctionName.EI.value:\n",
    "                self.acquisition_function = EI(mu, std, best, k)\n",
    "            case _:  # type: ignore\n",
    "                raise ValueError(\"Invalid acquisition function\")\n",
    "\n",
    "        return self.acquisition_function.__call__()\n",
    "\n",
    "    def _acquisition_optimization(\n",
    "        self, X: NDArray[np.float64], y: NDArray[np.float64]\n",
    "    ) -> NDArray[np.float64]:\n",
    "        # random search, generate random samples\n",
    "        Xsamples = npr.random(self.size)\n",
    "        Xsamples = Xsamples.reshape(len(Xsamples), 1)\n",
    "\n",
    "        # calculate the best surrogate score found so far\n",
    "        yhat, _ = self.surrogate(X)\n",
    "        best = max(yhat)\n",
    "        # calculate the mean and stdev via surrogate function\n",
    "        mu, std = self.surrogate(Xsamples)\n",
    "        # calculate the probability of improvement\n",
    "        scores = self._acquisition_function(\n",
    "            self._acquisition_function_name, mu, std, best, self.k\n",
    "        )\n",
    "\n",
    "        # locate the index of the largest scores\n",
    "        ix = np.argmax(scores)\n",
    "        return Xsamples[ix, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0t4GkKMEMcrH"
   },
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBPrQlhdMaER"
   },
   "source": [
    "### Regression\n",
    "---\n",
    "#### Questions:\n",
    "- How does the prior knowledge change the optimization?\n",
    "- How does the kernel change the optimization? (see here the [kernels](https://scikit-learn.org/stable/modules/gaussian_process.html#kernels-for-gaussian-processes))\n",
    "- How does the acquisition function affect the optimization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "wFdaATleMj3d",
    "outputId": "4ff31a96-07db-4f8b-a68e-0e7de49bda36"
   },
   "outputs": [],
   "source": [
    "size = 100\n",
    "BO = BayesianOptimization(\n",
    "    10,\n",
    "    size,\n",
    "    AcquisitionFunctionName.UCB,\n",
    "    ObjectiveFunctionName.Multimodal_3,\n",
    "    PriorName.Prior_1,\n",
    "    noise=0.1,\n",
    "    kernel=None,\n",
    "    k=0.1,\n",
    ")\n",
    "x, y = BO.__call__()\n",
    "BO.plot(x, y)\n",
    "# best result\n",
    "ix = np.argmax(y)\n",
    "print(\"Best Result: x=%.3f, y=%.3f\" % (x[ix][0], y[ix][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 100\n",
    "BO = BayesianOptimization(\n",
    "    20,\n",
    "    size,\n",
    "    AcquisitionFunctionName.LCB,\n",
    "    ObjectiveFunctionName.Multimodal_1,\n",
    "    PriorName.Prior_3,\n",
    "    noise=0.1,\n",
    "    kernel=None,\n",
    "    k=0.1,\n",
    ")\n",
    "x, y = BO.__call__()\n",
    "BO.plot(x, y)\n",
    "# best result\n",
    "ix = np.argmax(y)\n",
    "print(\"Best Result: x=%.3f, y=%.3f\" % (x[ix][0], y[ix][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change prior\n",
    "\n",
    "# The change in prior changes where the model samples the initial points and thus can make finding the global maximum harder\n",
    "# if the maximum is not in the prior distribution. In the long run this should not be inpactful\n",
    "# Given that prior 1 samples from the entire domain it should be the best prior to use\n",
    "size = 100\n",
    "for obj_fun in ObjectiveFunctionName:\n",
    "    for prior in PriorName:\n",
    "        BO = BayesianOptimization(\n",
    "            20,\n",
    "            size,\n",
    "            AcquisitionFunctionName.PI,\n",
    "            obj_fun,\n",
    "            prior,\n",
    "            noise=0.1,\n",
    "            kernel=None,\n",
    "            k=0.1,\n",
    "        )\n",
    "        x, y = BO.__call__()\n",
    "        BO.plot(x, y)\n",
    "        # best result\n",
    "        ix = np.argmax(y)\n",
    "        print(\"Best Result: x=%.3f, y=%.3f\" % (x[ix][0], y[ix][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change kernel\n",
    "# The kernel is used to model the covariance between the points in the domain. It doesn't seen to change the optimization process\n",
    "# significantly but this could simply be due to the fact that the functions are simple and the optimization process is not very complex\n",
    "kernels = [RBF(), DotProduct(), ExpSineSquared()]\n",
    "\n",
    "size = 100\n",
    "for obj_fun in ObjectiveFunctionName:\n",
    "    for kernel in kernels:\n",
    "        BO = BayesianOptimization(\n",
    "            20,\n",
    "            size,\n",
    "            AcquisitionFunctionName.PI,\n",
    "            obj_fun,\n",
    "            PriorName.Prior_1,\n",
    "            noise=0.1,\n",
    "            kernel=kernel,\n",
    "            k=0.1,\n",
    "        )\n",
    "        x, y = BO.__call__()\n",
    "        BO.plot(x, y)\n",
    "        # best result\n",
    "        ix = np.argmax(y)\n",
    "        print(\"Best Result: x=%.3f, y=%.3f\" % (x[ix][0], y[ix][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change acquisition function\n",
    "\n",
    "# The acquisition function is used to determine the next point to sample. The UCB function seems to be the best for this\n",
    "# while the EI function seems to be the worst. More generally UCB and LCB seem to focus directly around the maximum while\n",
    "# PI and EI seem to focus on exploring the domain more which means they might be better with more complex domanins\n",
    "size = 100\n",
    "for obj_fun in ObjectiveFunctionName:\n",
    "    for acq_fun in AcquisitionFunctionName:\n",
    "        BO = BayesianOptimization(\n",
    "            20,\n",
    "            size,\n",
    "            acq_fun,\n",
    "            obj_fun,\n",
    "            PriorName.Prior_1,\n",
    "            noise=0.1,\n",
    "            kernel=None,\n",
    "            k=0.1,\n",
    "        )\n",
    "        x, y = BO.__call__()\n",
    "        print(x.shape)\n",
    "        print(y.shape)\n",
    "        BO.plot(x, y)\n",
    "        # best result\n",
    "        ix = np.argmax(y)\n",
    "        print(\"Best Result: x=%.3f, y=%.3f\" % (x[ix][0], y[ix][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OH4P87xNNUCS"
   },
   "source": [
    "### BONUS\n",
    "\n",
    "You can now change the acquisition functions in the regression problem, adding a slack variable `k` as a hyperparameter. How does this variable affect the optimization problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cFDNlDGKNWJL"
   },
   "outputs": [],
   "source": [
    "# change k\n",
    "ks = [-10, -1, 0, 1, 10]\n",
    "\n",
    "size = 100\n",
    "for obj_fun in ObjectiveFunctionName:\n",
    "    for k in ks:\n",
    "        BO = BayesianOptimization(\n",
    "            20,\n",
    "            size,\n",
    "            AcquisitionFunctionName.PI,\n",
    "            obj_fun,\n",
    "            PriorName.Prior_1,\n",
    "            noise=0.1,\n",
    "            kernel=None,\n",
    "            k=k,\n",
    "        )\n",
    "        x, y = BO.__call__()\n",
    "        print(x.shape)\n",
    "        print(y.shape)\n",
    "        BO.plot(x, y)\n",
    "        # best result\n",
    "        ix = np.argmax(y)\n",
    "        print(\"Best Result: x=%.3f, y=%.3f\" % (x[ix][0], y[ix][0]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
