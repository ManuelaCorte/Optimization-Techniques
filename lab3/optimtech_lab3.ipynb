{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R18HTB2eEfAB"
   },
   "source": [
    "# Lab. 3: GD, Newton methods and BFGS\n",
    "\n",
    "## Introduction\n",
    "\n",
    "#### <u>The goal of this laboratory is to study the application of local search algorithms on different benchmark functions.</u>\n",
    "\n",
    "We will study three methods:\n",
    "- *Gradient Descent*\n",
    "- *Newton methods*\n",
    "- *BFGS*\n",
    "\n",
    "Moreover, we will study how their parameters change the behavior of these algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "Getting started: the following code cell contains the core functions that we will use. Hence, **remember to run it every time the runtime is reconnected**.\n",
    "\n",
    "It contains the three local search algorithms and a wrapper class called *OptFun* for the benchmark function.\n",
    "As regards the *OptFun* class, the constructor takes as input a benchmark function (we will see later what functions are available). The relevant methods  are 4:\n",
    "1.   *Minima*: return the minimum of the function. The position can be obtained by the parameter *position* and the function value from the *score* parameter.\n",
    "2.   *Bounds*: returns where the function is defined\n",
    "3.   *Heatmap*: show a heatmap of the function highlighting the points visited by the local search (use with 2d function)\n",
    "4.   *plot*: show the trend of the points visited by the local search (use with 1d function)\n",
    "5.   *trend*: show the best points find during the optmization process.\n",
    "\n",
    "Each instance of *OptFun* stores the history of the point at which the function has been evaluated. The history is never cleaned and can be obtained through *OptFun.history*. Hence, if you reuse the class instance remember to clean the history (*OptFun.history = list()*).\n",
    "\n",
    "---\n",
    "\n",
    "The benchmark functions available comes from the *benchmark_functions* library (imported as *bf*).\n",
    "Example of the functions that can be used are the *Hypersphere*, the *Rastrign* the *DeJong5* and the Keane.\n",
    "The complete list of functions available can be found at this [link](https://gitlab.com/luca.baronti/python_benchmark_functions) or printing *dir(bf)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_baC9HpMEg2N"
   },
   "source": [
    "#### Base code to run every time the runtime is reconnected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15478,
     "status": "ok",
     "timestamp": 1710428690046,
     "user": {
      "displayName": "Chiara Camilla Rambaldi Migliore",
      "userId": "14825705546977676123"
     },
     "user_tz": -60
    },
    "id": "bq-M_4uSEYed"
   },
   "outputs": [],
   "source": [
    "# from scipy.misc import derivative\n",
    "import inspect\n",
    "from copy import deepcopy\n",
    "from typing import Optional\n",
    "\n",
    "import benchmark_functions as bf\n",
    "import numdifftools as nd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.typing import NDArray\n",
    "from scipy.optimize import OptimizeResult, approx_fprime, minimize\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (22, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1710428690046,
     "user": {
      "displayName": "Chiara Camilla Rambaldi Migliore",
      "userId": "14825705546977676123"
     },
     "user_tz": -60
    },
    "id": "M7aMiVL4EiD8"
   },
   "outputs": [],
   "source": [
    "class OptFun:\n",
    "    def __init__(self, wf: bf.BenchmarkFunction) -> None:\n",
    "        self.f = wf\n",
    "        self.history: list[list[float]] = []\n",
    "\n",
    "    def __call__(self, x0: list[float]) -> float:\n",
    "        self.history.append(deepcopy(x0))\n",
    "        return self.f(x0)  # type: ignore\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return self.f.name()\n",
    "\n",
    "    def minima(self) -> list[bf.fil.Optimum]:\n",
    "        \"\"\"\n",
    "        Returns a list of Optimum objects of the known global minima. If there aren't any minima, an empty list value will be returned instead;\n",
    "\n",
    "        Returns:\n",
    "        - List of objects of class \"benchmark_functions.functions_info_loader.Optimum\"\n",
    "        - For each object:\n",
    "          - Access to 'position' parameter to get the axis values\n",
    "          - Access to 'score' parameter to get the value of the function\n",
    "        \"\"\"\n",
    "        return self.f.minima()\n",
    "\n",
    "    def bounds(self) -> list[tuple[float, float]]:\n",
    "        return self._convert_bounds(self.f.suggested_bounds())\n",
    "\n",
    "    def plot(self, fn: Optional[str] = None) -> None:\n",
    "        plt.clf()\n",
    "        ax1: plt.Axes\n",
    "        ax2: plt.Axes\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        f.suptitle(\"Benchmark Function: \" + self.name)\n",
    "\n",
    "        # heatmap\n",
    "        bounds_lower, bounds_upper = self.f.suggested_bounds()\n",
    "        x = np.linspace(bounds_lower[0], bounds_upper[0], 100)\n",
    "        if self.f.n_dimensions() > 1:\n",
    "            y = np.linspace(bounds_lower[1], bounds_upper[1], 100)\n",
    "            X, Y = np.meshgrid(x, y)\n",
    "            Z = np.asarray(\n",
    "                [\n",
    "                    [self.f((X[i][j], Y[i][j])) for j in range(len(X[i]))]\n",
    "                    for i in range(len(X))\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Function has only one dimension\")\n",
    "        ax1.contour(x, y, Z, 15, linewidths=0.5, colors=\"k\")\n",
    "        contour = ax1.contourf(x, y, Z, 15, cmap=\"viridis\", vmin=Z.min(), vmax=Z.max())\n",
    "        ax1.set_xlabel(\"x\")\n",
    "        ax1.set_ylabel(\"y\")\n",
    "        ax1.set_title(\"Heatmap\")\n",
    "        cbar = plt.colorbar(contour, ax=ax1)\n",
    "        cbar.set_label(\"z\")\n",
    "        if len(self.history) > 0:  # plot points\n",
    "            xdata = [x[0] for x in self.history]\n",
    "            ydata = [x[1] for x in self.history]\n",
    "\n",
    "            min_point: list[float] = self.minima()[0].position  # type: ignore\n",
    "            ax1.plot(min_point[0], min_point[1], \"Xb\", markersize=13)\n",
    "            ax1.plot(xdata[0], ydata[0], \"Xr\", markersize=13)\n",
    "            ax1.plot(xdata, ydata, \"or-\", markersize=5, linewidth=1.5)\n",
    "            ax1.plot(xdata[-1], ydata[-1], \"Pr\", markersize=13)\n",
    "\n",
    "        # convergence\n",
    "        values = [self.f(v) for v in self.history]\n",
    "        min: float = self.f.minima()[0].score  # type: ignore\n",
    "        ax2.plot(values)\n",
    "        ax2.axhline(min, color=\"r\", label=\"optimum\")\n",
    "        ax2.set_xlabel(\"Iterations\")\n",
    "        ax2.set_ylabel(\"f(x)\")\n",
    "        ax2.set_title(\"Function Evaluation\")\n",
    "        ax2.legend()\n",
    "\n",
    "        if fn is not None:\n",
    "            plt.savefig(fn, dpi=400)\n",
    "        plt.show()\n",
    "\n",
    "    def _convert_bounds(\n",
    "        self, bounds: tuple[list[float], list[float]]\n",
    "    ) -> list[tuple[float, float]]:\n",
    "        new_bounds = []\n",
    "        for i in range(len(bounds[0])):\n",
    "            new_bounds.append((bounds[0][i], bounds[1][i]))\n",
    "        return new_bounds\n",
    "\n",
    "    def current_calls(self):\n",
    "        return len(self.history)\n",
    "\n",
    "    def gradient(self, x: list[float]) -> NDArray[np.float64]:\n",
    "        return approx_fprime(x, self.f, epsilon=1.4901161193847656e-08)  # type: ignore\n",
    "\n",
    "\n",
    "def gradient_descent(\n",
    "    f: OptFun,\n",
    "    x0: list[float],\n",
    "    learn_rate: float,\n",
    "    n_iter: int = 50,\n",
    "    tolerance: float = 1e-06,\n",
    "    bounds: Optional[list[tuple[float, float]]] = None,\n",
    ") -> list[float]:\n",
    "    x = x0\n",
    "    f.history.append([v for v in x])\n",
    "    for _ in range(n_iter):\n",
    "        diff: NDArray[np.float64] = -learn_rate * f.gradient(x)\n",
    "        if np.abs(diff).all() <= tolerance:\n",
    "            break\n",
    "        x += diff\n",
    "        if bounds is not None:\n",
    "            for i in range(len(x)):\n",
    "                if x[i] < bounds[i][0]:\n",
    "                    x[i] = bounds[i][0]\n",
    "                elif x[i] > bounds[i][1]:\n",
    "                    x[i] = bounds[i][1]\n",
    "        f.history.append([v for v in x])\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def newton_method(\n",
    "    f: OptFun,\n",
    "    x0: list[float],\n",
    "    n_iter: int = 50,\n",
    "    tolerance: float = 1e-06,\n",
    "    bounds: Optional[list[tuple[float, float]]] = None,\n",
    ") -> list[float]:\n",
    "    x: NDArray[np.float64] = np.array(x0)\n",
    "    f.history.append(x.copy().tolist())\n",
    "    f_jacob = nd.Jacobian(f.f)\n",
    "    f_hess = nd.Hessian(f.f)\n",
    "    for _ in range(n_iter):\n",
    "        JA = f_jacob(x)\n",
    "        HA = np.linalg.inv(f_hess(x))\n",
    "        diff = -1 * np.dot(JA, HA)[0]\n",
    "        if np.all(np.abs(diff) <= tolerance):\n",
    "            break\n",
    "        x += diff\n",
    "        if bounds is not None:\n",
    "            for i in range(len(x)):\n",
    "                if x[i] < bounds[i][0]:\n",
    "                    x[i] = bounds[i][0]\n",
    "                elif x[i] > bounds[i][1]:\n",
    "                    x[i] = bounds[i][1]\n",
    "        f.history.append(x.copy().tolist())\n",
    "    return x.tolist()\n",
    "\n",
    "\n",
    "def bfgs(\n",
    "    f: OptFun,\n",
    "    x0: list[float],\n",
    "    eps: float,\n",
    "    maxiter: int,\n",
    "    bounds: Optional[list[tuple[float, float]]] = None,\n",
    ") -> OptimizeResult:\n",
    "    \"\"\"\n",
    "    Optimizes a function by using the BFGS algorithm.\n",
    "\n",
    "    - f: function to optimize, an instance of OptFun\n",
    "    - x0: starting point for the search process\n",
    "    - eps: step size for the update of the jacobian\n",
    "    - maxiter: maximum number of iterations\n",
    "    \"\"\"\n",
    "    return minimize(\n",
    "        f,\n",
    "        x0,\n",
    "        method=\"BFGS\",\n",
    "        jac=None,\n",
    "        bounds=bounds,\n",
    "        options={\n",
    "            \"gtol\": 1e-6,\n",
    "            \"norm\": float(\"inf\"),\n",
    "            \"eps\": eps,\n",
    "            \"maxiter\": maxiter,\n",
    "            \"disp\": False,\n",
    "            \"return_all\": True,\n",
    "            \"finite_diff_rel_step\": None,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1710428690046,
     "user": {
      "displayName": "Chiara Camilla Rambaldi Migliore",
      "userId": "14825705546977676123"
     },
     "user_tz": -60
    },
    "id": "jgYkF2IxLnvc"
   },
   "outputs": [],
   "source": [
    "def printClassInitArgs(class_obj: bf.BenchmarkFunction) -> None:\n",
    "    print(f\"{class_obj.name()}\")\n",
    "    signature = inspect.signature(class_obj.__init__).parameters\n",
    "    print(\"-------------------------------\")\n",
    "    for name, parameter in signature.items():\n",
    "        if name != \"opposite\":\n",
    "            print(\"Name: \", name, \"\\nDefault value:\", parameter.default)\n",
    "            # print(\"Annotation:\", parameter.annotation, \"\\nKind:\", parameter.kind)\n",
    "            print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tq-twZSWEmP7"
   },
   "source": [
    "# Exercises\n",
    "\n",
    "#### Solve the following exercises, and answer these questions at the end:\n",
    "\n",
    "- What is the difference in search cost (the number of function/derivative evaluations) between these methods?\n",
    "- Comparing these methods to the ones of the first laboratory, are they faster? Or find the optimal more efficiently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pyO5GZqALjtm",
    "outputId": "2b0fbeda-d513-4a8e-deae-43ab4a0c2c31"
   },
   "outputs": [],
   "source": [
    "# BE AWARE: check the arguments each benchmark function takes\n",
    "# if you're not sure, you can check the arguments by using the printClassInitArgs function\n",
    "\n",
    "\n",
    "printClassInitArgs(bf.Hypersphere())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxYV8nlSEn3N"
   },
   "source": [
    "## Exercise 1/3: GRADIENT DESCENT\n",
    "In this first exercise, we will focus on the Gradient Descent algorithm.\n",
    "The gradient descent function parameters are\n",
    "1.   *f*: the function to minimize\n",
    "2.   *x0*: the starting point\n",
    "3.   *learn_rate*: the learning rate\n",
    "4.   *n_iter*: maximum number of iterations\n",
    "5.   *tolerance*: the tolerance for finding the optimum (minimum step to move towards the optimum, if the step is lower than this threshold, the search stops)\n",
    "\n",
    "### Questions\n",
    "- How does the Learning Rate influence optimization?\n",
    "- How does tolerance influence the search?\n",
    "- Are the effects of these parameters the same across different functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_func = bf.Hypersphere()\n",
    "\n",
    "start_points = [[-2.0, -2.0], [2.0, 2.0]]\n",
    "learning_rates = [0.75, 0.01, 1e-3, 1e-4]\n",
    "n_iters = [10000]\n",
    "tolerance = 1e-03\n",
    "\n",
    "for x0 in start_points:\n",
    "    for lr in learning_rates:\n",
    "        for n_iter in n_iters:\n",
    "            func = OptFun(bench_func)\n",
    "            print(\"Function:\", func.name)\n",
    "            print(\"Starting point:\", x0)\n",
    "            print(\"Learning rate:\", lr)\n",
    "            print(\"Number of iterations:\", n_iter)\n",
    "            print(\"Function minimum point:\", func.minima()[0].position)\n",
    "            print(\"Function minimum value:\", func.minima()[0].score)\n",
    "\n",
    "            res = gradient_descent(func, x0, lr, n_iter, tolerance)\n",
    "            print(\"Gradient descent minimum point:\", res)\n",
    "            print(\"Gradient descent minimum value:\", func(res))\n",
    "            print(\"Gradient descent function evaluations:\", func.current_calls() - 1)\n",
    "            func.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1628,
     "status": "ok",
     "timestamp": 1710429579489,
     "user": {
      "displayName": "Chiara Camilla Rambaldi Migliore",
      "userId": "14825705546977676123"
     },
     "user_tz": -60
    },
    "id": "dfXdhh1CEpK5",
    "outputId": "48853e03-9e10-48c3-ca1c-f58598f491a0"
   },
   "outputs": [],
   "source": [
    "bench_func = bf.Rosenbrock()\n",
    "\n",
    "start_points = [[-2.0, -2.0], [2.0, 2.0]]\n",
    "learning_rates = [0.01, 0.001]\n",
    "n_iters = [10000]\n",
    "tolerance = 1e-06\n",
    "\n",
    "for x0 in start_points:\n",
    "    for lr in learning_rates:\n",
    "        for n_iter in n_iters:\n",
    "            func = OptFun(bench_func)\n",
    "            print(\"Function:\", func.name)\n",
    "            print(\"Starting point:\", x0)\n",
    "            print(\"Learning rate:\", lr)\n",
    "            print(\"Number of iterations:\", n_iter)\n",
    "            print(\"Function minimum point:\", func.minima()[0].position)\n",
    "            print(\"Function minimum value:\", func.minima()[0].score)\n",
    "\n",
    "            res = gradient_descent(\n",
    "                func, x0, lr, n_iter, tolerance, bounds=func.bounds()\n",
    "            )\n",
    "            print(\"Gradient descent minimum point:\", res)\n",
    "            print(\"Gradient descent minimum value:\", func(res))\n",
    "            print(\"Gradient descent function evaluations:\", func.current_calls() - 1)\n",
    "            func.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_func = bf.Rastrigin(2)\n",
    "\n",
    "start_points = [[-4.5, -4.5], [0.5, 0.5]]\n",
    "learning_rates = [0.001]\n",
    "n_iters = [10000]\n",
    "tolerance = 1e-06\n",
    "\n",
    "for x0 in start_points:\n",
    "    for lr in learning_rates:\n",
    "        for n_iter in n_iters:\n",
    "            func = OptFun(bench_func)\n",
    "            print(\"Function:\", func.name)\n",
    "            print(\"Starting point:\", x0)\n",
    "            print(\"Learning rate:\", lr)\n",
    "            print(\"Number of iterations:\", n_iter)\n",
    "            print(\"Function minimum point:\", func.minima()[0].position)\n",
    "            print(\"Function minimum value:\", func.minima()[0].score)\n",
    "\n",
    "            res = gradient_descent(func, x0, lr, n_iter, tolerance)\n",
    "            print(\"Gradient descent minimum point:\", res)\n",
    "            print(\"Gradient descent minimum value:\", func(res))\n",
    "            print(\"Gradient descent function evaluations:\", func.current_calls() - 1)\n",
    "            func.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_func = bf.PichenyGoldsteinAndPrice()\n",
    "\n",
    "start_points = [[0.0, -2.0]]\n",
    "learning_rates = [0.005]\n",
    "n_iters = [1000]\n",
    "tolerance = 1e-06\n",
    "\n",
    "for x0 in start_points:\n",
    "    for lr in learning_rates:\n",
    "        for n_iter in n_iters:\n",
    "            func = OptFun(bench_func)\n",
    "            print(\"Function:\", func.name)\n",
    "            print(\"Starting point:\", x0)\n",
    "            print(\"Learning rate:\", lr)\n",
    "            print(\"Number of iterations:\", n_iter)\n",
    "            print(\"Function minimum point:\", func.minima()[0].position)\n",
    "            print(\"Function minimum value:\", func.minima()[0].score)\n",
    "\n",
    "            res = gradient_descent(func, x0, lr, n_iter, tolerance)\n",
    "            print(\"Gradient descent minimum point:\", res)\n",
    "            print(\"Gradient descent minimum value:\", func(res))\n",
    "            print(\"Gradient descent function evaluations:\", func.current_calls() - 1)\n",
    "            func.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important  note: learning rate crucial not only for the convergence of the algorithm but also for the stability of the algorithm(too high learning rate can lead to overflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFYKsQGFEroe"
   },
   "source": [
    "## Exercise 2/3: NEWTON METHOD\n",
    "In this exercise, we will see the Newton method.\n",
    "The Newton Method function parameters are\n",
    "1.   *f*: the function to minimize\n",
    "2.   *x0*: the starting point\n",
    "3.   *n_iter*: maximum number of iterations\n",
    "4.   *tolerance*: the tolerance for finding the optimum (minimum step to move towards the optimum, if the step is lower than this threshold, the search stops)\n",
    "\n",
    "### Questions\n",
    "- Is it faster to converge with respect to GD?\n",
    "- How does tolerance influence the search?\n",
    "- Are the results similar across different functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_func = bf.PichenyGoldsteinAndPrice()\n",
    "\n",
    "start_points = [[0.0, -2.0]]\n",
    "n_iters = [1000]\n",
    "tolerance = 2\n",
    "\n",
    "for x0 in start_points:\n",
    "    for n_iter in n_iters:\n",
    "        func = OptFun(bench_func)\n",
    "        print(\"Function:\", func.name)\n",
    "        print(\"Starting point:\", x0)\n",
    "        print(\"Number of iterations:\", n_iter)\n",
    "        print(\"Function minimum point:\", func.minima()[0].position)\n",
    "        print(\"Function minimum value:\", func.minima()[0].score)\n",
    "\n",
    "        res = newton_method(func, x0, n_iter, tolerance, bounds=func.bounds())\n",
    "        print(\"Newton method minimum point:\", res)\n",
    "        print(\"Newton method minimum value:\", func(res))\n",
    "        print(\"Newton method function evaluations:\", func.current_calls() - 1)\n",
    "        func.plot()\n",
    "\n",
    "print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1595,
     "status": "ok",
     "timestamp": 1710429682163,
     "user": {
      "displayName": "Chiara Camilla Rambaldi Migliore",
      "userId": "14825705546977676123"
     },
     "user_tz": -60
    },
    "id": "T5fM817yEthm",
    "outputId": "511985b4-3e34-4379-9c4a-74ed790fed85"
   },
   "outputs": [],
   "source": [
    "bench_func = bf.Rosenbrock()\n",
    "\n",
    "start_points = [[2.0, 2.0], [-1.0, 1.5]]\n",
    "n_iters = [1000, 10000]\n",
    "tolerance = 1e-09\n",
    "\n",
    "for x0 in start_points:\n",
    "    for n_iter in n_iters:\n",
    "        func = OptFun(bench_func)\n",
    "        print(\"Function:\", func.name)\n",
    "        print(\"Starting point:\", x0)\n",
    "        print(\"Number of iterations:\", n_iter)\n",
    "        print(\"Function minimum point:\", func.minima()[0].position)\n",
    "        print(\"Function minimum value:\", func.minima()[0].score)\n",
    "\n",
    "        res = newton_method(func, x0, n_iter, tolerance, bounds=func.bounds())\n",
    "        print(\"Newton method minimum point:\", res)\n",
    "        print(\"Newton method minimum value:\", func(res))\n",
    "        print(\"Newton method function evaluations:\", func.current_calls() - 1)\n",
    "        func.plot()\n",
    "\n",
    "print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FHn9dWcEv51"
   },
   "source": [
    "## Exercise 3/3: BFGS OPTIMIZATION\n",
    "In this exercise, we will focus on the BFGS optimization algorithm.\n",
    "The BFGS function parameters are\n",
    "1.   *f*: the function to minimize\n",
    "2.   *x0*: the starting point\n",
    "3.   *eps*: step size for the update of the jacobian\n",
    "4.   *max_iter*: maximum number of iterations\n",
    "\n",
    "### Questions\n",
    "- What does it happen by varying these parameters?\n",
    "- How do they influence the evolution process?\n",
    "- What is the difference between BFGS and L-BFGS? Hint: you have to change the BFGS function, calling the right method to minimize. See [here](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-lbfgsb.html#optimize-minimize-lbfgsb) the parameters available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1319,
     "status": "ok",
     "timestamp": 1710430268153,
     "user": {
      "displayName": "Chiara Camilla Rambaldi Migliore",
      "userId": "14825705546977676123"
     },
     "user_tz": -60
    },
    "id": "NzwPDv2nExRP",
    "outputId": "ff92897a-5469-44c5-9039-534a0ebc97b0"
   },
   "outputs": [],
   "source": [
    "bench_func = bf.Rosenbrock()\n",
    "\n",
    "start_points = [[-1.5, -1.5], [2.0, 2.0]]\n",
    "epsilons = [0.01]\n",
    "n_iters = [1000]\n",
    "tolerance = 1e-6\n",
    "\n",
    "for x0 in start_points:\n",
    "    for eps in epsilons:\n",
    "        for n_iter in n_iters:\n",
    "            func = OptFun(bench_func)\n",
    "            print(\"Function:\", func.name)\n",
    "            print(\"Starting point:\", x0)\n",
    "            print(\"Epsilon:\", eps)\n",
    "            print(\"Number of iterations:\", n_iter)\n",
    "            print(\"Function minimum point:\", func.minima()[0].position)\n",
    "            print(\"Function minimum value:\", func.minima()[0].score)\n",
    "\n",
    "            res = bfgs(func, x0, lr, n_iter, func.bounds())\n",
    "            print(\"BFGS minimum point:\", res.x)\n",
    "            print(\"BFGS minimum value:\", res.fun)\n",
    "            print(\"BFGS function evaluations:\", func.current_calls() - 1)\n",
    "            func.plot()\n",
    "\n",
    "\n",
    "print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwDWeAvOEz35"
   },
   "source": [
    "## Final questions\n",
    "- What is the difference in search cost (the number of function/derivative evaluations) between these methods?\n",
    "- Comparing these methods to the ones of the first laboratory, are they faster? Or find the optimal more efficiently?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agTUxhHEEyZ1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
