{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vU000j5BD2fA"
   },
   "source": [
    "## `Course evaluation information`\n",
    "\n",
    "- Concisely note down your observations for each lab from now on.\n",
    "- You can take notes inside the notebooks, or in separate PDFs.\n",
    "- Either way, be ready to show the work you putted into each lab, including the experiments and the learning outcomes.\n",
    "- During the exam, you will be asked to show your notes for some of the labs at random with a brief discussion on its content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjbcedKxD3_G"
   },
   "source": [
    "# Lab. 1: Local search\n",
    "\n",
    "## Introduction\n",
    "\n",
    "#### <u>The goal is to study the application of local search algorithms on different benchmark functions.</u>\n",
    "\n",
    "We will see the following methods:\n",
    "- *Grid Search*\n",
    "- *Random Search*\n",
    "- *Powell*\n",
    "- *Nelder Mead*\n",
    "\n",
    "Moreover, we will study how their parameters change the behavior of these algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "Getting started: the following code cell contains the core functions that we will use. Hence, **remember to run it every time the runtime is reconnected**.\n",
    "\n",
    "It contains the three local search algorithms and a wrapper class called *OptFun* for the benchmark function.\n",
    "As regards the *OptFun* class, the constructor takes as input a benchmark function (we will see later what functions are available). The relevant methods  are 4:\n",
    "1.   *Minima*: return the minimum of the function. The position can be obtained by the parameter *position* and the function value from the *score* parameter.\n",
    "2.   *Bounds*: returns where the function is defined\n",
    "3.   *Heatmap*: show a heatmap of the function highlighting the points visited by the local search (use with 2d function)\n",
    "4.   *plot*: show the best points find during the optmization process.\n",
    "\n",
    "Each instance of *OptFun* stores the history of the point at which the function has been evaluated. The history is never cleaned and can be obtained through *OptFun.history*. Hence, if you reuse the class instance remember to clean the history (*OptFun.history = list()*).\n",
    "\n",
    "---\n",
    "\n",
    "The benchmark functions available comes from the *benchmark_functions* library (imported as *bf*).\n",
    "Example of the functions that can be used are the *Hypersphere*, the *Rastrign* the *DeJong5* and the Keane.\n",
    "The complete list of functions available can be found at this [link](https://gitlab.com/luca.baronti/python_benchmark_functions) or you can print it with *dir(bf)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8uoNBLJD5iC"
   },
   "source": [
    "#### Base code to run every time the runtime is reconnected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v31Sul4BDqx5"
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "from copy import deepcopy\n",
    "from typing import Optional\n",
    "\n",
    "import benchmark_functions as bf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.typing import NDArray\n",
    "from scipy.optimize import OptimizeResult, minimize\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (22, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esjtvqg5D7FA"
   },
   "outputs": [],
   "source": [
    "class OptFun:\n",
    "    def __init__(self, wf: bf.BenchmarkFunction) -> None:\n",
    "        self.f = wf\n",
    "        self.history: list[list[float]] = []\n",
    "\n",
    "    def __call__(self, x0: list[float]) -> float:\n",
    "        self.history.append(deepcopy(x0))\n",
    "        return self.f(x0)  # type: ignore\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return self.f.name()\n",
    "\n",
    "    def minima(self) -> list[bf.fil.Optimum]:\n",
    "        return self.f.minima()\n",
    "\n",
    "    def bounds(self) -> list[tuple[float, float]]:\n",
    "        return self._convert_bounds(self.f.suggested_bounds())\n",
    "\n",
    "    def found_minimum(self) -> list[float]:\n",
    "        minimum = self.history[0]\n",
    "        for x in self.history:\n",
    "            if self.f(x) < self.f(minimum):  # type: ignore\n",
    "                minimum = x\n",
    "        return minimum\n",
    "\n",
    "    def plot(self, fn: Optional[str] = None) -> None:\n",
    "        plt.clf()\n",
    "        ax1: plt.Axes\n",
    "        ax2: plt.Axes\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        f.suptitle(\"Benchmark Function: \" + self.name)\n",
    "\n",
    "        # heatmap\n",
    "        bounds_lower, bounds_upper = self.f.suggested_bounds()\n",
    "        x = np.linspace(bounds_lower[0], bounds_upper[0], 100)\n",
    "        if self.f.n_dimensions() > 1:\n",
    "            y = np.linspace(bounds_lower[1], bounds_upper[1], 100)\n",
    "            X, Y = np.meshgrid(x, y)\n",
    "            Z = np.asarray(\n",
    "                [\n",
    "                    [self.f((X[i][j], Y[i][j])) for j in range(len(X[i]))]\n",
    "                    for i in range(len(X))\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Function has only one dimension\")\n",
    "        ax1.contour(x, y, Z, 15, linewidths=0.5, colors=\"k\")\n",
    "        contour = ax1.contourf(x, y, Z, 15, cmap=\"viridis\", vmin=Z.min(), vmax=Z.max())\n",
    "        ax1.set_xlabel(\"x\")\n",
    "        ax1.set_ylabel(\"y\")\n",
    "        ax1.set_title(\"Heatmap\")\n",
    "        cbar = plt.colorbar(contour, ax=ax1)\n",
    "        cbar.set_label(\"z\")\n",
    "        if len(self.history) > 0:  # plot points\n",
    "            xdata = [x[0] for x in self.history]\n",
    "            ydata = [x[1] for x in self.history]\n",
    "            ax1.plot(xdata, ydata, \"or-\", markersize=2, linewidth=2)\n",
    "            # plot function minimum\n",
    "            minima = self.f.minima()[0]\n",
    "            ax1.plot(minima.position[0], minima.position[1], \"or\", markersize=10)\n",
    "\n",
    "        # convergence\n",
    "        values = [self.f(v) for v in self.history]\n",
    "        min: float = self.f.minima()[0].score  # type: ignore\n",
    "        ax2.plot(values)\n",
    "        ax2.axhline(min, color=\"r\", label=\"optimum\")\n",
    "        ax2.set_xlabel(\"Iterations\")\n",
    "        ax2.set_ylabel(\"f(x)\")\n",
    "        ax2.set_title(\"Function Evaluation\")\n",
    "        ax2.legend()\n",
    "\n",
    "        if fn is not None:\n",
    "            plt.savefig(fn, dpi=400)\n",
    "        plt.show()\n",
    "\n",
    "    def _convert_bounds(\n",
    "        self, bounds: tuple[list[float], list[float]]\n",
    "    ) -> list[tuple[float, float]]:\n",
    "        new_bounds: list[tuple[float, float]] = []\n",
    "        for i in range(len(bounds[0])):\n",
    "            new_bounds.append((bounds[0][i], bounds[1][i]))\n",
    "        return new_bounds\n",
    "\n",
    "    def current_calls(self):\n",
    "        return len(self.history)\n",
    "\n",
    "\n",
    "def grid_search(\n",
    "    f: OptFun, step_size: Optional[float] = None, number_of_steps: Optional[int] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Optimizes a function by using the grid_search algorithm.\n",
    "\n",
    "    - f: function to optimize, an instance of OptFun\n",
    "    - step_size: the step size\n",
    "    - number_of_steps: the total number of steps\n",
    "    \"\"\"\n",
    "    bounds = f.bounds()\n",
    "    if step_size is not None:\n",
    "        for x in np.arange(bounds[0][0], bounds[0][1], step_size):\n",
    "            for y in np.arange(bounds[1][0], bounds[1][1], step_size):\n",
    "                f([x, y])\n",
    "    elif number_of_steps is not None:\n",
    "        for x in np.linspace(\n",
    "            bounds[0][0], bounds[0][1], int(np.floor(np.sqrt(number_of_steps)))\n",
    "        ):\n",
    "            for y in np.linspace(\n",
    "                bounds[1][0], bounds[1][1], int(np.floor(np.sqrt(number_of_steps)))\n",
    "            ):\n",
    "                f([x, y])\n",
    "    else:\n",
    "        print(\"Please provide at least the step_size or the number of steps\")\n",
    "\n",
    "\n",
    "def random_search(f: OptFun, n_samples_drawn: int):\n",
    "    \"\"\"\n",
    "    Optimizes a function by using the random_search algorithm.\n",
    "\n",
    "    - f: function to optimize, an instance of OptFun\n",
    "    - number_of_steps: the total number of steps\n",
    "    \"\"\"\n",
    "    bounds = f.bounds()\n",
    "    for _ in range(n_samples_drawn):\n",
    "        x = np.random.uniform(bounds[0][0], bounds[0][1])\n",
    "        y = np.random.uniform(bounds[1][0], bounds[1][1])\n",
    "        f([x, y])\n",
    "\n",
    "\n",
    "def powell(\n",
    "    f: OptFun,\n",
    "    x0: list[float],\n",
    "    maxiter: int,\n",
    "    initial_directions: Optional[NDArray[np.float64]] = None,\n",
    ") -> OptimizeResult:\n",
    "    \"\"\"\n",
    "    Optimizes a function by using the Powell algorithm.\n",
    "\n",
    "    - f: function to optimize, an instance of OptFun\n",
    "    - x0: starting point for the search process\n",
    "    - maxiter: maximum number of iterations\n",
    "    \"\"\"\n",
    "    bounds = f.bounds()\n",
    "    results: OptimizeResult = minimize(\n",
    "        fun=f,\n",
    "        x0=list(x0),\n",
    "        method=\"powell\",\n",
    "        bounds=bounds,\n",
    "        options={\n",
    "            \"ftol\": 1e-4,\n",
    "            \"maxfev\": None,\n",
    "            \"maxiter\": maxiter,\n",
    "            \"direc\": initial_directions,\n",
    "            \"return_all\": True,\n",
    "        },\n",
    "    )\n",
    "    return results\n",
    "\n",
    "\n",
    "def nelder_mead(f: OptFun, x0: list[float], maxiter: int) -> OptimizeResult:\n",
    "    \"\"\"\n",
    "    Optimizes a function by using the Nelder-Mead algorithm.\n",
    "\n",
    "    - f: function to optimize, an instance of OptFun\n",
    "    - x0: starting point for the search process\n",
    "    - maxiter: maximum number of iterations\n",
    "    \"\"\"\n",
    "    bounds = f.bounds()\n",
    "    results = minimize(\n",
    "        f,\n",
    "        x0,\n",
    "        method=\"Nelder-Mead\",\n",
    "        tol=None,\n",
    "        bounds=bounds,\n",
    "        options={\n",
    "            \"maxfev\": None,\n",
    "            \"maxiter\": maxiter,\n",
    "            \"disp\": False,\n",
    "            \"return_all\": True,\n",
    "            \"initial_simplex\": None,\n",
    "            \"xatol\": 0.000,\n",
    "            \"fatol\": 0.000,\n",
    "            \"adaptive\": False,\n",
    "        },\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printClassInitArgs(class_obj: bf.BenchmarkFunction):\n",
    "    signature = inspect.signature(class_obj.__init__).parameters\n",
    "    print(\"-------------------------------\")\n",
    "    for name, parameter in signature.items():\n",
    "        print(\"Name: \", name, \"\\nDefault value:\", parameter.default)\n",
    "        # print(\"Annotation:\", parameter.annotation, \"\\nKind:\", parameter.kind)\n",
    "        print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-sBepeND-VN"
   },
   "source": [
    "# Exercises\n",
    "\n",
    "#### Solve the following exercises, and answer these questions at the end:\n",
    "\n",
    "- How the benchmark functions influence the optimization algorithms? There is an algorithm which is always better than the other?\n",
    "- The choiche of the parameters is influenced by the function to optimize? And how the algorithms are influenced by the parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BE AWARE: check the arguments each benchmark function takes and ignore the \"opposite\" argument\n",
    "# if you're not sure, you can check the arguments by using the printClassInitArgs function\n",
    "printClassInitArgs(bf.DeJong5())\n",
    "\n",
    "printClassInitArgs(bf.Hypersphere())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hk1O3ORID_r1"
   },
   "source": [
    "## Exercise 1/4: GRID SEARCH\n",
    "In this first exercise we will use grid search as a search algorithm\n",
    "\n",
    "### Questions\n",
    "- How does the step size influence the quality of the best point obtained?\n",
    "- How does the step size influence the search cost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = [\n",
    "    bf.Hypersphere(2),\n",
    "    bf.Rastrigin(2),\n",
    "    bf.Ackley(2),\n",
    "]\n",
    "num_steps = [10, 100, 1000]\n",
    "\n",
    "for benchmark in benchmarks:\n",
    "    for steps in num_steps:\n",
    "        func = OptFun(benchmark)\n",
    "        grid_search(func, number_of_steps=steps)\n",
    "\n",
    "        print(\"Benchmark function: \", func.name)\n",
    "        print(\"Num steps: \", steps)\n",
    "        print(\"Real minimum: \", func.minima()[0])\n",
    "        print(\"Found minimum: \", func.found_minimum())\n",
    "        func.plot(f\"imgs/gs_{func.name}_step_size_{steps}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more points we sample (tighter discretization), the more likely we are to find the global minimum. However, the cost of the search increases with the number of points sampled. Since we are doing a global search, there is no basin of attraction to exploit which can be a good thing to avoid local minima, but also a bad thing because we are not exploiting any information on the function itself.\n",
    "\n",
    "Moreover, given we are equally sampling the space, the shape of the function is not important (there is no harder function to optimize wrt another), the only additional complexity is given by a more expensive function to evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rubg9cVGEEBm"
   },
   "source": [
    "## Exercise 2/4: RANDOM SEARCH\n",
    "\n",
    "In this exercise we will use Random Search to search for the optimum\n",
    "\n",
    "### Questions\n",
    "- How does the number of samples drawn affect the search?\n",
    "- How does this method compare to Grid Search? What are the advantages and disadvantages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 981
    },
    "id": "GbBnfe1xECTk",
    "outputId": "0a3c4f96-9814-4b2c-bc26-4c8c4fc5547a"
   },
   "outputs": [],
   "source": [
    "benchmarks = [\n",
    "    bf.Hypersphere(2),\n",
    "    bf.Rastrigin(2),\n",
    "    bf.Rosenbrock(2),\n",
    "    bf.Ackley(2),\n",
    "    bf.DeJong5(),\n",
    "    bf.Keane(2),\n",
    "]\n",
    "n_samples = [10, 100, 500]\n",
    "\n",
    "for benchmark in benchmarks:\n",
    "    for n_samples_drawn in n_samples:\n",
    "        func = OptFun(benchmark)\n",
    "        random_search(func, n_samples_drawn)\n",
    "\n",
    "        print(\"Benchmark function: \", func.name)\n",
    "        print(\"Number of samples: \", n_samples_drawn)\n",
    "        print(\"Real minimum: \", func.minima()[0])\n",
    "        print(\"Found minimum: \", func.found_minimum())\n",
    "        func.plot(f\"imgs/rs_{func.name}_samples_{n_samples_drawn}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random search is a stochastic method, hence it can happen that sampling more points does not guarantee a better result. However, the more points we sample, the more likely we are to find the global minimum. The cost of the search increases with the number of points sampled.\n",
    "\n",
    "Similarly to grid search, the shape of the function is not important\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUAupSZJEIEA"
   },
   "source": [
    "## Exercise 3/4: POWELL OPTIMIZATION\n",
    "\n",
    "In this exercise we will focus on the Powel optimization algorithm.\n",
    "\n",
    "### Questions\n",
    "- What happens when varying the parameters of the algorithm?\n",
    "- How they influence the optimization process?\n",
    "- The effects of these parameters is the same across different functions?\n",
    "- How does this algorithm compare to the previous?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 981
    },
    "id": "gO9gu3uiEJ2c",
    "outputId": "c642e427-9e29-46cd-bcda-e2a488453b14"
   },
   "outputs": [],
   "source": [
    "benchmark = bf.Hypersphere(2)\n",
    "initial_points = [[4.0, -4.0], [1.0, 1.5], [0.5, -0.5]]\n",
    "max_iters = [1, 10, 100]\n",
    "\n",
    "for x_0 in initial_points:\n",
    "    for max_iter in max_iters:\n",
    "        func = OptFun(benchmark)\n",
    "        powell(func, x_0, max_iter)\n",
    "\n",
    "        print(\"Benchmark function: \", func.name)\n",
    "        print(\"Initial point: \", x_0)\n",
    "        print(\"Max iterations: \", max_iter)\n",
    "        print(\"Real minimum: \", func.minima()[0])\n",
    "        print(\"Found minimum: \", func.found_minimum())\n",
    "        func.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = bf.Rastrigin(2)\n",
    "initial_points = [[4.0, -4.0], [1.0, 1.5], [0.5, -0.5]]\n",
    "max_iters = [1, 10, 100]\n",
    "\n",
    "for x_0 in initial_points:\n",
    "    for max_iter in max_iters:\n",
    "        func = OptFun(benchmark)\n",
    "        powell(func, x_0, max_iter)\n",
    "\n",
    "        print(\"Benchmark function: \", func.name)\n",
    "        print(\"Initial point: \", x_0)\n",
    "        print(\"Max iterations: \", max_iter)\n",
    "        print(\"Real minimum: \", func.minima()[0])\n",
    "        print(\"Found minimum: \", func.found_minimum())\n",
    "        func.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = OptFun(bf.Ackley(2))\n",
    "axis = [np.eye(2), np.array([[-1, 0], [0, -1]])]\n",
    "initial_point = [-10.0, -10.0]\n",
    "iters = 100\n",
    "\n",
    "for directions in axis:\n",
    "    powell(func, initial_point, iters, directions)\n",
    "    print(\"Benchmark function: \", func.name)\n",
    "    print(\"Initial point: \", initial_point)\n",
    "    print(\"Max iterations: \", iters)\n",
    "    print(\"Real minimum: \", func.minima()[0])\n",
    "    print(\"Found minimum: \", func.found_minimum())\n",
    "    func.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = bf.Rosenbrock(2)\n",
    "initial_points = [[2.0, -2.0], [1.0, 1.5], [0.25, -0.25]]\n",
    "max_iters = [1, 10, 100]\n",
    "\n",
    "for x_0 in initial_points:\n",
    "    for max_iter in max_iters:\n",
    "        func = OptFun(benchmark)\n",
    "        powell(func, x_0, max_iter)\n",
    "\n",
    "        print(\"Benchmark function: \", func.name)\n",
    "        print(\"Initial point: \", x_0)\n",
    "        print(\"Max iterations: \", max_iter)\n",
    "        print(\"Real minimum: \", func.minima()[0])\n",
    "        print(\"Found minimum: \", func.found_minimum())\n",
    "        func.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = bf.Hypersphere(2)\n",
    "initial_points = [[4.0, -4.0], [1.0, 1.5], [0.5, -0.5]]\n",
    "max_iters = [1, 10, 100]\n",
    "\n",
    "for x_0 in initial_points:\n",
    "    for max_iter in max_iters:\n",
    "        func = OptFun(benchmark)\n",
    "        powell(func, x_0, max_iter)\n",
    "\n",
    "        print(\"Benchmark function: \", func.name)\n",
    "        print(\"Initial point: \", x_0)\n",
    "        print(\"Max iterations: \", max_iter)\n",
    "        print(\"Real minimum: \", func.minima()[0])\n",
    "        print(\"Found minimum: \", func.found_minimum())\n",
    "        func.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = bf.Ackley(2)\n",
    "initial_points = [[10.0, 10.0], [10.0, 5.0], [2.0, 0.5]]\n",
    "max_iters = [1, 10, 100]\n",
    "\n",
    "for x_0 in initial_points:\n",
    "    for max_iter in max_iters:\n",
    "        func = OptFun(benchmark)\n",
    "        powell(func, x_0, max_iter)\n",
    "\n",
    "        print(\"Benchmark function: \", func.name)\n",
    "        print(\"Initial point: \", x_0)\n",
    "        print(\"Max iterations: \", max_iter)\n",
    "        print(\"Real minimum: \", func.minima()[0])\n",
    "        print(\"Found minimum: \", func.found_minimum())\n",
    "        func.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = bf.Keane(2)\n",
    "initial_points = [[10.0, 10.0], [10.0, 5.0], [2.0, 0.5]]\n",
    "max_iters = [1, 10, 100, 1000]\n",
    "\n",
    "for x_0 in initial_points:\n",
    "    for max_iter in max_iters:\n",
    "        func = OptFun(benchmark)\n",
    "        powell(func, x_0, max_iter)\n",
    "\n",
    "        print(\"Benchmark function: \", func.name)\n",
    "        print(\"Initial point: \", x_0)\n",
    "        print(\"Max iterations: \", max_iter)\n",
    "        print(\"Real minimum: \", func.minima()[0])\n",
    "        print(\"Found minimum: \", func.found_minimum())\n",
    "        func.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = bf.DeJong5()\n",
    "initial_points = [[10.0, 10.0], [10.0, 5.0], [2.0, 0.5]]\n",
    "max_iters = [1, 10, 100]\n",
    "\n",
    "for x_0 in initial_points:\n",
    "    for max_iter in max_iters:\n",
    "        func = OptFun(benchmark)\n",
    "        powell(func, x_0, max_iter)\n",
    "\n",
    "        print(\"Benchmark function: \", func.name)\n",
    "        print(\"Initial point: \", x_0)\n",
    "        print(\"Max iterations: \", max_iter)\n",
    "        print(\"Real minimum: \", func.minima()[0])\n",
    "        print(\"Found minimum: \", func.found_minimum())\n",
    "        func.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKMsWBW4EMop"
   },
   "source": [
    "## Exercise 4/4: NELDER MEAD OPTIMIZATION\n",
    "\n",
    "In this exercise we will focus on the Nelder Mead optimization algorithm.\n",
    "Similar to the previous exercise, answer the following questions:\n",
    "\n",
    "### Questions\n",
    "- What happens when varying the parameters of the algorithm?\n",
    "- How they influence the optimization process?\n",
    "- The effects of these parameters is the same across different functions?\n",
    "- How does this algorithm compare to the previous?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 981
    },
    "id": "0pXtC5qQEK3X",
    "outputId": "1f9d0d65-ab2b-4e32-9d2b-1a7a25c3a300"
   },
   "outputs": [],
   "source": [
    "benchmark_function = bf.Hypersphere(2)\n",
    "initial_points = [[4.0, -4.0], [1.75, 1.0], [0.5, 0.5]]\n",
    "max_iters = [10, 100, 1000]\n",
    "\n",
    "for x_0 in initial_points:\n",
    "    for max_iter in max_iters:\n",
    "        func = OptFun(benchmark_function)\n",
    "        nelder_mead(func, x_0, max_iter)\n",
    "\n",
    "        print(\"Benchmark function: \", func.name)\n",
    "        print(\"Initial guess: \", x_0)\n",
    "        print(\"Max iterations: \", max_iter)\n",
    "        print(\"Real minimum: \", func.minima()[0])\n",
    "        print(\"Found minumun: \", func.found_minimum())\n",
    "        func.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_function = bf.Rastrigin(2)\n",
    "initial_points = [[4.0, -4.0], [1.75, 1.0], [0.25, 0.25]]\n",
    "max_iters = [10, 100, 1000]\n",
    "\n",
    "for x_0 in initial_points:\n",
    "    for max_iter in max_iters:\n",
    "        func = OptFun(benchmark_function)\n",
    "        nelder_mead(func, x_0, max_iter)\n",
    "\n",
    "        print(\"Benchmark function: \", func.name)\n",
    "        print(\"Initial guess: \", x_0)\n",
    "        print(\"Max iterations: \", max_iter)\n",
    "        print(\"Real minimum: \", func.minima()[0])\n",
    "        print(\"Found minumun: \", func.found_minimum())\n",
    "        func.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_function = bf.Rosenbrock(2)\n",
    "initial_points = [[2.0, -2.0], [0.25, -0.25], [1.0, 1.5]]\n",
    "max_iters = [10, 100, 1000]\n",
    "\n",
    "for x_0 in initial_points:\n",
    "    for max_iter in max_iters:\n",
    "        func = OptFun(benchmark_function)\n",
    "        nelder_mead(func, x_0, max_iter)\n",
    "\n",
    "        print(\"Benchmark function: \", func.name)\n",
    "        print(\"Initial guess: \", x_0)\n",
    "        print(\"Max iterations: \", max_iter)\n",
    "        print(\"Real minimum: \", func.minima()[0])\n",
    "        print(\"Found minumun: \", func.found_minimum())\n",
    "        func.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_function = bf.Ackley(2)\n",
    "initial_points = [[20.0, -25.0], [10.0, 5.0], [2.0, 0.5]]\n",
    "max_iters = [10, 100, 1000]\n",
    "\n",
    "for x_0 in initial_points:\n",
    "    for max_iter in max_iters:\n",
    "        func = OptFun(benchmark_function)\n",
    "        nelder_mead(func, x_0, max_iter)\n",
    "\n",
    "        print(\"Benchmark function: \", func.name)\n",
    "        print(\"Initial guess: \", x_0)\n",
    "        print(\"Max iterations: \", max_iter)\n",
    "        print(\"Real minimum: \", func.minima()[0])\n",
    "        print(\"Found minumun: \", func.found_minimum())\n",
    "        func.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_function = bf.Keane(2)\n",
    "initial_points = [[10.0, 10.0], [5.0, 7.0], [1.5, 0.75]]\n",
    "max_iters = [10, 100, 1000]\n",
    "\n",
    "for x_0 in initial_points:\n",
    "    for max_iter in max_iters:\n",
    "        func = OptFun(benchmark_function)\n",
    "        nelder_mead(func, x_0, max_iter)\n",
    "\n",
    "        print(\"Benchmark function: \", func.name)\n",
    "        print(\"Initial guess: \", x_0)\n",
    "        print(\"Max iterations: \", max_iter)\n",
    "        print(\"Real minimum: \", func.minima()[0])\n",
    "        print(\"Found minumun: \", func.found_minimum())\n",
    "        func.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests\n",
    "\n",
    "benchmark_function = bf.Ackley(2)\n",
    "\n",
    "x_0 = [25.0, 25.0]\n",
    "iterations = [1000]\n",
    "for max_iter in iterations:\n",
    "    func = OptFun(benchmark_function)\n",
    "    res = nelder_mead(func, x_0, max_iter)\n",
    "\n",
    "    func.plot()\n",
    "\n",
    "    print(\"Initial guess: \", x_0)\n",
    "    print(\"Real minimum: \", func.minima()[0])\n",
    "    print(\"Found minumun: \", func.found_minimum())\n",
    "    print(\"Iterations: \", max_iters)\n",
    "\n",
    "x_0 = [20.0, 20.0]\n",
    "iterations = [1000]\n",
    "for max_iter in iterations:\n",
    "    func = OptFun(benchmark_function)\n",
    "    res = nelder_mead(func, x_0, max_iter)\n",
    "\n",
    "    func.plot()\n",
    "\n",
    "    print(\"Initial guess: \", x_0)\n",
    "    print(\"Real minimum: \", func.minima()[0])\n",
    "    print(\"Found minumun: \", func.found_minimum())\n",
    "    print(\"Iterations: \", max_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously the closer the initial guess if to the minimum the faster the convergence and the best the obtained result.\n",
    "\n",
    "Note how since the function is symmetric, if the starting points are symmetric wrt the minimum the shape of the function of the convergence process is the same albeit at different scales. (The point (1.75, 1.5) that isn't symmetric has a different shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBgMyrgREPo7"
   },
   "source": [
    "## Final questions\n",
    "- How the benchmark functions influence the optimization algorithms? There is an algorithm which is always better than the other?\n",
    "- The choiche of the parameters is influenced by the function to optimize? And how the algorithm are influenced by the parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x67hRUq7ENoO"
   },
   "outputs": [],
   "source": [
    "# TODO: compare the different optimization algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "L8uoNBLJD5iC",
    "Q-sBepeND-VN",
    "hk1O3ORID_r1",
    "Rubg9cVGEEBm",
    "KUAupSZJEIEA",
    "JKMsWBW4EMop"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
